# ds210_final_project
BU CDS DS210 Final Rust Graph Analysis Project

CDS DS 210 Final Project Report
Idan Gutkind

Data Source:
The data I used is from Kaggle. It is a Twitter Communication Network dataset with the names of 1000 users and dummy variables to show if they follow each of the other users. The graph is directed and labeled and has 1000 nodes, and 50153 edges. Huawei uses this Twitter data as part of its social network analysis for customer analysis in its marketing strategy. The company employs social network analysis tools and techniques in its research and development efforts. The data collected from Twitter, along with Facebook and Instagram, is used to enhance Huawei's business positions and promote its products through social media.

Data Cleaning:
The first step was data cleaning and preparation. I converted the Excel spreadsheet into a txt file. Then I systematically assigned each person a unique number ID from 0 to 999. This created nodes to represent each person. Next I went through the list of connections between people. Whenever two people knew each other, I created a connection or "edge" between their number IDs in the system. For example, if the original data showed that "Mary" knew "John", I would create an edge between the node representing Mary and the node representing John. By going through all the connections, I effectively converted the readable name data into an abstract representation using numbered nodes and edges in a network structure. This prepared graph could then be analyzed using graph analysis learned in class to understand patterns and insights about the relationships between people. The text-based edge list format made it possible to run calculations and metrics on the connections. By doing the work of data cleaning and conversion up front, I enabled deeper analysis on this people network down the line.
Next, I had to read the text-file (named t_twitter.txt) that I had created, and convert it into a Vector of tuples in Rust for further processing (using Rust's file I/O capabilities). A Vector in Rust allows storing data in a resizable array format. I needed to get the edge data into a structure I could manipulate. I defined a tuple struct that held a start node ID and end node ID for each edge. As I parsed the text file, I created many tuple instances capturing each edge and inserted them into the Vector. This gave me a clean representation of all connections from the original Excel sheet. Finally I defined my own custom Graph struct in Rust to hold nodes and edges. I implemented various functions for this Graph to add nodes, add edges between nodes, fetch neighbors of a node etc.

Graph Analysis
After I had built my Graph data structure of all the people and connections, I wanted to filter down to the most influential people. To do this, I implemented the PageRank algorithm which assigns a score to each node based on the number and weight of incoming edges. By running PageRank, I got a ranking of nodes - essentially people - by importance. I took the top 50 highest ranked nodes to make a filtered graph.
I made a new Graph keeping only the top 50 nodes and any edges between them from the original graph. The graph containing the Top 50 edges, could vary each time we run our code, as it contains a finite amount of randomness.) This gave me a subset graph focused on the well-connected people in the network. I could analyze just this filtered down graph to understand patterns between the influencers. On this graph, I ran a Breadth First Search algorithm starting from specified root nodes. This allowed me to traverse all nodes reachable from, for example, the most influential people. By recording the nodes visited in order, I could see how connections flowed outward through this elite sub-graph. Ultimately I calculated distances between all pairs of the top 50 nodes with shortest paths. This final metric exposed insights into how closely connected various influencers were based on traversing friendship links. The exact top 50 varied due to randomness in PageRank, but the process revealed core insights each time into this key network.


Conclusion
After applying my graph analysis process, I could see that the distances between the top 50 nodes were very small - mostly between 1 and 2 steps, with a few being 3. This is a surprisingly tiny distance compared to other typical social networks. The top nodes tend to be very tightly interconnected - they either connect directly or have lots of mutual connections. This contrasts with two random people in a network, who would likely be many more hops apart. The short path lengths arise because prominent people in this network cluster together. My algorithmic analysis quantitatively demonstrated that the top ranked people form a dense mesh of connectivity between themselves.


